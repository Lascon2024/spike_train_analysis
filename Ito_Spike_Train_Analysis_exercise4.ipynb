{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spike Train Analysis: exercise 4\n",
    "\n",
    "This notebook covers the following topics:\n",
    "\n",
    "* Higher-order correlations; Compound Poisson Process as a model of population correlations\n",
    "* Complexity distribution\n",
    "* Analysis of higher order correlations with the SPADE method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import quantities as pq\n",
    "import elephant.spike_train_surrogates as surr\n",
    "import elephant.statistics as stats\n",
    "import elephant.spike_train_generation as stocmod\n",
    "import elephant.spade as spade\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Higher-order correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Compound Poisson Process (CPP)\n",
    "\n",
    "The Compund Poisson Process (CPP) is a versatile model of parallel spike trains correlated by a well-defined correlation structure. Here, we use the CPP to generate various types of correlated data, that we will later analyse.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Generate correlated parallel spike trains (e.g. 100) by means of the CPP (see `help(stocmod.cpp)`). \n",
    "* Plot the data: are correlations visible in the raster plot by eye? \n",
    "* Plot also the population histogram and the histogram of its values (the *complexity distribution*): is the correlation visible there? Try different firing rates and amplitude distributions.\n",
    "* Discuss the role of the amplitude distribution in determining the correlation structure among the parallel spike trains.\n",
    "* Discuss the relation between the amplitude distribution and the complexity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike train parameters\n",
    "num_units = 100      \n",
    "rate = 2 * pq.Hz \n",
    "trial_duration = 10 * pq.s\n",
    "# - parameters for the CPP model\n",
    "assembly_size = 20\n",
    "p_coinc = 0.05\n",
    "jitter = 0 * pq.ms\n",
    "\n",
    "# Analysis parameters\n",
    "binsize = 1 * pq.ms\n",
    "\n",
    "# Construct the amplitude distribution for the CPP\n",
    "amp_dist = np.zeros(num_units + 1) \n",
    "amp_dist[1] = 1. - p_coinc\n",
    "amp_dist[assembly_size] = p_coinc\n",
    "\n",
    "# Generate the CPP data\n",
    "cpp = stocmod.cpp(rate, amp_dist, trial_duration, shift=jitter)\n",
    "\n",
    "# Compute the population histogram and the complexity distribution\n",
    "complexity_cpp = stats.Complexity(cpp, bin_size=binsize, binary=True)\n",
    "pophist = complexity_cpp.time_histogram\n",
    "dist_cpp = complexity_cpp.pdf().flatten().magnitude\n",
    "dist_cpp = np.concatenate([dist_cpp, np.zeros(num_units - len(dist_cpp))])\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "ax1.set_xlabel('Time (ms)')\n",
    "ax1.set_ylabel('Unit ID')\n",
    "for i, st in enumerate(cpp):\n",
    "    ax1.plot(st.rescale('ms').magnitude, [i + 1] * len(st), '.', ms=1, color='black')\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 3, sharex=ax1)\n",
    "ax2.set_xlabel('Time (ms)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.plot(pophist.times.rescale('ms'), pophist[:, 0], lw=0.5)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 2)\n",
    "ax3.set_xlabel('Complexity')\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.bar(np.arange(len(amp_dist)), amp_dist, label='amplitude distrib.')\n",
    "ax3.legend()\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "ax4.set_xlabel('Complexity')\n",
    "ax4.set_ylabel('Probability')\n",
    "ax4.plot(np.arange(num_units), dist_cpp, '.-', label='complexity distrib.')\n",
    "ax4.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Investigating correlations by the complexity distribution\n",
    "\n",
    "The complexity distribution carries information about the probability of observing any number of synchronous spikes in the data. Such synchrony can be due to chance, or to correlations in the spiking activity. It will most of the time be a sum of the two.\n",
    "\n",
    "Generating control, independent data and comparing their complexity distribution to that of the original data can be an effective way to assess the presence of higher-order correlations (see Gr√ºn et al (2008)).\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Generate control data in terms of an independent CPP with same marginal statistics (number of neurons, firing rate)\n",
    "* Compare the two amplitude distributions (e.g. by plotting their difference): are correlations visible in the correlated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of surrogates\n",
    "surr_sts = []\n",
    "\n",
    "for st in cpp:\n",
    "    surr_sts.append(surr.randomise_spikes(st)[0])\n",
    "\n",
    "# Computation of the Complexity Distributions\n",
    "\n",
    "def compute_complexity_distribution(spiketrains, bin_size):\n",
    "    n = len(spiketrains)\n",
    "    complexity = stats.Complexity(spiketrains, bin_size=bin_size, binary=True)\n",
    "    dist = complexity.pdf().flatten().magnitude\n",
    "    return np.concatenate([dist, np.zeros(n - len(dist))])\n",
    "\n",
    "dist_surr = compute_complexity_distribution(surr_sts, binsize)\n",
    "\n",
    "dist_diff = dist_cpp - dist_surr\n",
    "\n",
    "# Plot the difference of the complexity distributions of the correlated and independent CPP\n",
    "fig = plt.figure(figsize = (12, 4))\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_xlabel('Complexity')\n",
    "ax1.set_ylabel('Probability')\n",
    "ax1.plot(np.arange(num_units), dist_cpp, color='blue', label=\"CPP\")\n",
    "ax1.plot(np.arange(num_units), dist_surr, color='red', label=\"surrogate\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_xlabel('Complexity')\n",
    "ax2.set_ylabel('Probability difference')\n",
    "ax2.plot(np.arange(num_units), dist_diff)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to investigate the sensitivity of this measure to the binsize parameter, proceeding with the computation of complexity for different parameter setting, both of the correlated data and of the surrogates.\n",
    "In particular it is possible to consider the coordinates of the second local maximum of the difference beetween complexity of correlated data and surrogate (purple line). This value is supposed to depend on the order of correlation of the data.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Is it possible to infer a relation between the binsize and the time-precision of the synchrony embedded in the cpp model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computate the complexity distributions of CPP and surrogates for different binsizes \n",
    "# and store the results in matrices\n",
    "binsizes = np.arange(1, 31) * pq.ms\n",
    "complexity_cpp_matrix = []\n",
    "complexity_surr_matrix = []\n",
    "diff_complexity_matrix = []\n",
    "peak_pos = []\n",
    "for i, binsize in enumerate(binsizes):\n",
    "    complexity_cpp = compute_complexity_distribution(cpp, binsize)\n",
    "    complexity_surr = compute_complexity_distribution(surr_sts, binsize)\n",
    "    complexity_cpp_matrix.append(complexity_cpp)\n",
    "    complexity_surr_matrix.append(complexity_surr)\n",
    "\n",
    "    diff_complexity = complexity_cpp - complexity_surr\n",
    "    diff_complexity_matrix.append(diff_complexity)\n",
    "\n",
    "    trough_pos = np.argmin(diff_complexity)\n",
    "    peak_pos.append(trough_pos + np.argmax(diff_complexity[trough_pos:]))\n",
    "\n",
    "complexity_cpp_matrix = np.array(complexity_cpp_matrix)\n",
    "complexity_surr_matrix = np.array(complexity_surr_matrix)\n",
    "diff_complexity_matrix = np.array(diff_complexity_matrix)\n",
    "\n",
    "complexities = np.arange(complexity_cpp_matrix.shape[1])\n",
    "\n",
    "# Plot the complexity matrices\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "ax1.set_title('CPP complexity')\n",
    "ax1.set_xlabel('Complexity')\n",
    "ax1.set_ylabel('Binsize')\n",
    "im = ax1.pcolormesh(complexities, binsizes, complexity_cpp_matrix)\n",
    "plt.colorbar(im)\n",
    "\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "ax2.set_title('Surrogate complexity')\n",
    "ax2.set_xlabel('Complexity')\n",
    "ax2.set_ylabel('Binsize')\n",
    "im = ax2.pcolormesh(complexities, binsizes, complexity_surr_matrix)\n",
    "plt.colorbar(im)\n",
    "\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "ax3.set_title('Difference of complexities')\n",
    "ax3.set_xlabel('Complexity')\n",
    "ax3.set_ylabel('Binsize')\n",
    "im = ax3.pcolormesh(diff_complexity_matrix)\n",
    "plt.colorbar(im)\n",
    "ax3.plot(peak_pos, binsizes, 'm')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SPADE\n",
    "SPADE (Spike PAttern Detection and Evaluation) is a method to detect spatio-temporal patterns (STPs) in massively parallel spike trains.\n",
    "It consists of a mining step, detection of candidate patterns, and two statistical test steps, evaluating significance of the patterns (see Quaglio et al. 2017).\n",
    "The pattern here is defined as exact spike synchrony across neurons participating in an assembly, without any temporal delay or jitter.\n",
    "\n",
    "### 3.3.1 Insertion of STPs into independent artificial data\n",
    "Again we generate appropriate artificial data in order to understand the method through applications.\n",
    "Here we generate the data by combining two different CPPs: the first modeling the neurons participating in an assembly, the second modeling independent data for the remaining neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike train parameters\n",
    "num_units = 100      \n",
    "rate = 2 * pq.Hz \n",
    "trial_duration = 10 * pq.s\n",
    "# - parameters for the CPP model\n",
    "assembly_size = 10\n",
    "p_coinc = 0.05\n",
    "jitter = 2 * pq.ms\n",
    "\n",
    "# Analysis parameters\n",
    "binsize_comp_dist = 1 * pq.ms\n",
    "\n",
    "# Constduct the amplitude distribution for the CPP \n",
    "amp_dist_sip = np.zeros(assembly_size + 1) \n",
    "amp_dist_sip[1] = 1. - p_coinc\n",
    "amp_dist_sip[assembly_size] = p_coinc\n",
    "\n",
    "# Generate the CPP data\n",
    "cpp_sp = stocmod.cpp(rate, amp_dist_sip, trial_duration, shift=jitter)\n",
    "\n",
    "# Generate independent data and combine the two data sets\n",
    "amp_dist_indep = [0, 1] + [0] * (num_units - assembly_size - 1)\n",
    "cpp_sp.extend(stocmod.cpp(rate, amp_dist_indep, trial_duration))\n",
    "\n",
    "# Compute the population histogram and the complexity distribution\n",
    "complexity_cpp = stats.Complexity(cpp_sp, bin_size=binsize_comp_dist, binary=True)\n",
    "pophist = complexity_cpp.time_histogram\n",
    "dist_cpp = complexity_cpp.pdf().flatten().magnitude\n",
    "dist_cpp = np.concatenate([dist_cpp, np.zeros(num_units - len(dist_cpp))])\n",
    "\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = plt.subplot(2, 2, 1)\n",
    "ax1.set_xlabel('Time (ms)')\n",
    "ax1.set_ylabel('Unit ID')\n",
    "for i, st in enumerate(cpp_sp):\n",
    "    ax1.plot(st.rescale('ms').magnitude, [i + 1] * len(st), '.', ms=1, color='black')\n",
    "\n",
    "ax2 = plt.subplot(2, 2, 3, sharex=ax1)\n",
    "ax2.set_xlabel('Time (ms)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.plot(pophist.times.rescale('ms'), pophist[:, 0], lw=0.5)\n",
    "\n",
    "ax3 = plt.subplot(2, 2, 2)\n",
    "ax3.set_xlabel('Complexity')\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.bar(np.arange(len(amp_dist_sip)), amp_dist_sip, label='amplitude distrib.')\n",
    "ax3.legend()\n",
    "\n",
    "ax4 = plt.subplot(2, 2, 4)\n",
    "ax4.set_xlabel('Complexity')\n",
    "ax4.set_ylabel('Probability')\n",
    "ax4.plot(np.arange(num_units), dist_cpp, '.-', label='complexity distrib.')\n",
    "ax4.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Application of SPADE\n",
    "\n",
    "Now we move to the application of the method to the artificially generated data. The goal is to detect the embedded patterns with SPADE, and visualize them.\n",
    "If needed, look at the documentation of SPADE.\n",
    "* Apply the method and plot the detected patterns.\n",
    "* Display the output of the method and examine the contents.\n",
    "* Is there a relation between `binsize` and the time precision of the synchrony embedded in the CPP (i.e., `jitter` parameter in the previous code cell)?\n",
    "* Play with the parameters of the method. For example, what happens if the number of surrogates is set to zero (no statistical evaluation)? What happens if `psr_param` is set to `None` (no pattern set reduction)?\n",
    "* (Optional) Try to inject a second pattern, by generating a second CPP with the appropriate number of components, and apply the method to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis parameters\n",
    "binsize = 1 * pq.ms\n",
    "winlen = 5\n",
    "n_surr = 20\n",
    "significance_level = 0.01\n",
    "\n",
    "# Apply SPADE to the data\n",
    "spade_result = spade.spade(cpp_sp, binsize, winlen, output_format='patterns', n_surr=n_surr, alpha=significance_level, psr_param=[0,0,0])\n",
    "\n",
    "# Plot the result\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Unit index')\n",
    "\n",
    "i_pattern_unit = []\n",
    "for i_pattern, pattern in enumerate(spade_result['patterns']):\n",
    "    for i, t in enumerate(pattern['times'].magnitude):\n",
    "        spikes_times = [t] + [t+l for l in pattern['lags'].magnitude]\n",
    "        if i == 0:\n",
    "            ax.plot(spikes_times, pattern['neurons'], 'o-', color=f'C{i_pattern}', label=f'Pattern {i_pattern}: {pattern[\"signature\"]}')\n",
    "        else:\n",
    "            ax.plot(spikes_times, pattern['neurons'], 'o-', color=f'C{i_pattern}')\n",
    "    i_pattern_unit.extend(pattern['neurons'])\n",
    "i_pattern_unit = np.unique(i_pattern_unit)\n",
    "\n",
    "for i_unit in i_pattern_unit:\n",
    "    st = cpp_sp[i_unit]\n",
    "    ax.plot(st.rescale('ms'), [i_unit]*len(st), '.k')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spade_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
